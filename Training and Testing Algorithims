import pandas as pd
import numpy as np
from sklearn.metrics import f1_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load clean5.csv into a dataframe
filename = "clean5.csv"
dataframe_6 = pd.read_csv(filename)

# Selecting numeric columns and dropping NA values
clean4 = dataframe_6.select_dtypes(include=[np.number]).dropna()

# Defining feature columns
feature_columns = [
    "Duration",
    "Date completed label",
    "Inclusion_gender_label",
]

# Calculating descriptive statistics
mean_values = clean4[feature_columns].mean()
std_dev_values = clean4[feature_columns].std()
min_values = clean4[feature_columns].min()
max_values = clean4[feature_columns].max()

# Calculating VIF
vif_data = pd.DataFrame()
vif_data["feature"] = feature_columns
vif_data["VIF"] = [
    variance_inflation_factor(clean4[feature_columns].values, i)
    for i in range(len(feature_columns))
]

# Output descriptive statistics
print("Mean Values:\n", mean_values)
print("Standard Deviation Values:\n", std_dev_values)
print("Minimum Values:\n", min_values)
print("Maximum Values:\n", max_values)
print("VIF:\n", vif_data)

# Split the data into train and test sets
X = clean4[feature_columns]
y = clean4["Inclusion_gender_label"]

# Ensure that y has at least two classes
if len(y.unique()) < 2:
    raise ValueError(
        "The target variable has less than 2 unique classes, which is insufficient for training the models."
    )

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Initialize the models
log_reg = LogisticRegression(random_state=42)
perceptron = Perceptron(random_state=42)
svm = SVC(random_state=42)

# Train the models
log_reg.fit(X_train, y_train)
perceptron.fit(X_train, y_train)
svm.fit(X_train, y_train)

# Predict on the test set
y_pred_log_reg = log_reg.predict(X_test)
y_pred_perceptron = perceptron.predict(X_test)
y_pred_svm = svm.predict(X_test)

# Calculate the accuracy
accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
accuracy_perceptron = accuracy_score(y_test, y_pred_perceptron)
accuracy_svm = accuracy_score(y_test, y_pred_svm)

# Print the accuracies
print(
    "Logistic Regression train accuracy:",
    accuracy_score(y_train, log_reg.predict(X_train)),
)
print("Logistic Regression test accuracy:", accuracy_log_reg)
print(
    "Perceptron train accuracy:", accuracy_score(y_train, perceptron.predict(X_train))
)
print("Perceptron test accuracy:", accuracy_perceptron)
print("SVM train accuracy:", accuracy_score(y_train, svm.predict(X_train)))
print("SVM test accuracy:", accuracy_svm)
# Calculate the F1 score for each model
f1_log_reg = f1_score(y_test, y_pred_log_reg, average="weighted")
f1_perceptron = f1_score(y_test, y_pred_perceptron, average="weighted")
f1_svm = f1_score(y_test, y_pred_svm, average="weighted")

# Print the F1 scores
print("Logistic Regression F1 score:", f1_log_reg)
print("Perceptron F1 score:", f1_perceptron)
print("SVM F1 score:", f1_svm)
